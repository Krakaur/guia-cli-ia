# Troubleshooting: Casos Reales Documentados

Esta gu√≠a contiene soluciones a problemas **reales** encontrados durante meses de uso intensivo de CLIs de IA en investigaci√≥n acad√©mica y desarrollo de sistemas multi-agente.

---

## üìã Tabla de Contenidos

- [Problemas de Encoding](#problemas-de-encoding)
- [Timeouts y Latencia](#timeouts-y-latencia)
- [Parsing de JSON](#parsing-de-json)
- [Limitaciones de PowerShell](#limitaciones-de-powershell)
- [Procesos Hu√©rfanos](#procesos-hu√©rfanos)
- [Modelos y Autenticaci√≥n](#modelos-y-autenticaci√≥n)

---

## üî§ Problemas de Encoding

### Caso 1: DeepSeek crash con emojis (Windows)

**S√≠ntoma:**
```
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f680'
```

**Contexto:** DeepSeek genera emojis en respuesta, PowerShell crashea.

**Soluci√≥n permanente:**
```powershell
[Environment]::SetEnvironmentVariable("PYTHONIOENCODING", "utf-8", "User")
```

**Soluci√≥n temporal:**
```powershell
$env:PYTHONIOENCODING="utf-8"; deepseek -q "prompt" -r
```

**Lecci√≥n:** En Windows, SIEMPRE configurar encoding UTF-8 para CLIs Python.

---

### Caso 2: Acentos espa√±oles en prompts largos

**S√≠ntoma:** CLI acepta prompt pero respuesta muestra caracteres corruptos.

**Causa:** Archivo temporal creado sin encoding UTF-8.

**Soluci√≥n:**
```powershell
# Al crear archivos temporales
$prompt | Out-File -FilePath temp.txt -Encoding UTF8

# NO usar:
$prompt > temp.txt  # Usa encoding del sistema (no UTF-8)
```

---

## ‚è±Ô∏è Timeouts y Latencia

### Caso 3: Gemini timeout variable (15-68 segundos)

**Contexto:** Mismo prompt, diferente latencia seg√∫n hora del d√≠a.

**Datos emp√≠ricos:**
- Latencia m√≠nima: 15 segundos
- Latencia m√°xima: 68 segundos
- Promedio: ~30 segundos

**Configuraci√≥n recomendada:**
```python
GEMINI_TIMEOUT = 90000  # 90 segundos (margen de seguridad)
```

**Lecci√≥n:** No asumir latencias consistentes. Usar timeouts 3x promedio.

---

### Caso 4: Claude Code timeout en PDFs grandes

**S√≠ntoma:** `claude -p "analiza PDF..."` timeout despu√©s de 120 segundos.

**Causa:** PDF de 225 p√°ginas excede contexto procesable en tiempo razonable.

**Soluci√≥n:** Truncar PDF antes de enviar.

```python
from PyPDF2 import PdfReader, PdfWriter

def truncate_pdf(input_path, max_pages=7):
    reader = PdfReader(input_path)
    writer = PdfWriter()
    
    for i in range(min(max_pages, len(reader.pages))):
        writer.add_page(reader.pages[i])
    
    output_path = f"truncated_{input_path.name}"
    with open(output_path, 'wb') as f:
        writer.write(f)
    
    return output_path
```

**Lecci√≥n:** Preprocesar documentos grandes antes de enviar a LLM.

---

## üìÑ Parsing de JSON

### Caso 5: JSON envuelto en markdown (todos los CLIs)

**Problema:** Todos los CLIs decoran JSON:

```markdown
Aqu√≠ est√° el resultado:

```json
{
  "status": "OK"
}
```

Espero que sea √∫til!
```

**Soluci√≥n multi-estrategia:**

```python
import re
import json

def extract_json(text: str) -> dict:
    text = text.strip()
    
    # Estrategia 1: Parseo directo
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # Estrategia 2: Buscar bloque markdown
    match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Estrategia 3: Buscar primer { hasta √∫ltimo }
    first = text.find('{')
    last = text.rfind('}')
    if first != -1 and last != -1:
        try:
            return json.loads(text[first:last+1])
        except json.JSONDecodeError:
            pass
    
    # Estrategia 4: Buscar primer [ hasta √∫ltimo ]
    first = text.find('[')
    last = text.rfind(']')
    if first != -1 and last != -1:
        try:
            return json.loads(text[first:last+1])
        except json.JSONDecodeError:
            pass
    
    raise ValueError(f"No JSON v√°lido en: {text[:200]}...")
```

**Lecci√≥n:** Nunca confiar en formato de respuesta. Siempre extraer JSON con fallbacks.

---

## üíª Limitaciones de PowerShell

### Caso 6: Command line length limit (8191 chars)

**S√≠ntoma:** Prompt largo se trunca silenciosamente.

**Contexto:** Windows PowerShell limita comandos a ~8191 caracteres.

**Detecci√≥n:**
```powershell
$cmd = "claude -p '$largoPrompt' --allowedTools ''"
if ($cmd.Length -gt 8000) {
    Write-Warning "Prompt puede truncarse!"
}
```

**Soluci√≥n 1: Archivo temporal + stdin**
```powershell
$prompt | Out-File -FilePath temp.txt -Encoding UTF8
Get-Content temp.txt | claude --allowedTools ""
Remove-Item temp.txt
```

**Soluci√≥n 2: Python wrapper**
```python
import subprocess

def call_cli_safe(cli, prompt, timeout=120):
    with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', 
                                     suffix='.txt', delete=False) as f:
        f.write(prompt)
        temp_file = f.name
    
    try:
        cmd = f'type "{temp_file}" | {cli}'
        result = subprocess.run(cmd, shell=True, capture_output=True,
                               text=True, timeout=timeout, encoding='utf-8')
        return result.stdout
    finally:
        Path(temp_file).unlink()
```

**Lecci√≥n:** En Windows, NO pasar prompts largos como argumentos de CLI.

---

## üëª Procesos Hu√©rfanos

### Caso 7: Gemini deja procesos hu√©rfanos

**S√≠ntoma:** Despu√©s de usar Gemini 10+ veces, memoria del sistema alta.

**Diagn√≥stico:**
```powershell
Get-Process | Where-Object {$_.ProcessName -like "*node*"} | 
    Measure-Object -Property WS -Sum

# Resultado: 2GB+ en procesos node.js
```

**Causa:** Gemini CLI inicia proceso Node.js que no siempre termina correctamente.

**Soluci√≥n: Protocolo de higiene**

```python
# ANTES de cada llamada a Gemini
list_sessions()  # Ver procesos activos
force_terminate(pid)  # Matar hu√©rfanos

# Ejecutar Gemini
result = gemini(prompt)

# DESPU√âS de cada llamada (SIEMPRE)
force_terminate(gemini_pid)
```

**Lecci√≥n:** CLIs basados en Node.js requieren limpieza activa de procesos.

---

## üîê Modelos y Autenticaci√≥n

### Caso 8: Copilot - modelos premium no funcionan en batch

**S√≠ntoma:**
```bash
copilot -p "test" --model gpt-5.2 -s --allow-all-tools
# Error: "Run in interactive mode to enable this model"
```

**Causa:** Modelos premium requieren activaci√≥n interactiva previa.

**Soluci√≥n:**
```bash
# Paso 1: Activar interactivamente (una vez)
copilot --model gpt-5.2
# [Esperar activaci√≥n, Ctrl+C para salir]

# Paso 2: Ahora funciona en batch
copilot -p "test" --model gpt-5.2 -s --allow-all-tools
```

**Modelos que NO requieren activaci√≥n:**
- ‚úÖ `gpt-4.1`
- ‚úÖ `gpt-4o`

**Lecci√≥n:** Documentar qu√© modelos funcionan en batch vs interactivo.

---

### Caso 9: Token GitHub expirado

**S√≠ntoma:** Copilot falla con 401 Unauthorized.

**Diagn√≥stico:**
```powershell
# Verificar si token existe
echo $env:COPILOT_GITHUB_TOKEN

# Verificar si es v√°lido (via GitHub API)
$headers = @{
    "Authorization" = "Bearer $env:COPILOT_GITHUB_TOKEN"
}
Invoke-RestMethod -Uri "https://api.github.com/user" -Headers $headers
```

**Soluci√≥n:**
1. Ir a: https://github.com/settings/tokens
2. Regenerar token
3. Actualizar variable:
```powershell
[Environment]::SetEnvironmentVariable("COPILOT_GITHUB_TOKEN", "nuevo-token", "User")
```

**Lecci√≥n:** Tokens tienen expiraci√≥n. Implementar verificaci√≥n peri√≥dica.

---

## üîÑ Modo Interactivo Inesperado

### Caso 10: Gemini entra en modo interactivo (batch corrupto)

**S√≠ntoma:** Script se detiene esperando input del usuario.

**Causa:** Prompt >500 caracteres activa confirmaci√≥n interactiva.

**Detecci√≥n:**
```python
if len(prompt) > 500:
    logger.warning(f"Prompt largo ({len(prompt)} chars) - riesgo de modo interactivo")
```

**Soluci√≥n:** Mantener prompts <200 caracteres para Gemini en automatizaci√≥n.

```python
def truncate_prompt_safe(prompt, max_length=200):
    if len(prompt) <= max_length:
        return prompt
    
    # Truncar inteligentemente (no a media palabra)
    truncated = prompt[:max_length].rsplit(' ', 1)[0]
    return truncated + "..."
```

**Lecci√≥n:** Cada CLI tiene umbrales diferentes para activar modo interactivo.

---

## üìä Resumen de Mejores Pr√°cticas

| Problema | Prevenci√≥n |
|----------|-----------|
| Encoding | SIEMPRE configurar UTF-8 en Windows |
| Timeouts | Usar 3x latencia promedio |
| JSON | Implementar multi-estrategia de parsing |
| Prompts largos | Usar archivos temporales + stdin |
| Procesos hu√©rfanos | Limpieza sistem√°tica antes/despu√©s |
| Modelos premium | Activar interactivamente primero |
| Tokens | Verificar validez peri√≥dicamente |

---

## üî¨ Metodolog√≠a de Troubleshooting

### Paso 1: Reproducir con caso m√≠nimo

```bash
# NO hacer:
copilot -p "$complejo_prompt_con_variables" --model $modelo -s --allow-all-tools

# S√ç hacer:
copilot -p "hola" --model gpt-4.1 -s --allow-all-tools
```

### Paso 2: Aislar variable

¬øFalla con:
- ‚úÖ Prompt espec√≠fico ‚Üí Problema en contenido
- ‚úÖ Modelo espec√≠fico ‚Üí Problema en autenticaci√≥n/disponibilidad
- ‚úÖ Siempre ‚Üí Problema en instalaci√≥n/config

### Paso 3: Documentar emp√≠ricamente

```markdown
## Issue: Copilot timeout con modelo X

**Intentos:**
1. Timeout 30s ‚Üí Falla
2. Timeout 60s ‚Üí Falla
3. Timeout 120s ‚Üí ‚úÖ Funciona

**Conclusi√≥n:** Modelo X requiere timeout m√≠nimo 120s.
```

---

**Contribuye:** Si encuentras nuevos casos, abre un [issue](https://github.com/Krakaur/guia-cli-ia/issues) con:
- S√≠ntoma exacto
- Comando ejecutado
- Output del error
- Soluci√≥n que funcion√≥

---

**√öltima actualizaci√≥n:** Enero 2026
**Autor:** Dr. Hans Krakaur
